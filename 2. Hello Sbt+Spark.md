## 2. Scala+Sbt+Spark
#### Установить Standalone spark
https://github.com/ribentrop/spark-hello-world/blob/master/spark-standalone.md
#### Работа в консоли Spark Shell
Самый простой способ заставить Spark что-то поделать, это запустить Spark Shell.
При этом запустится интерпретатор Scala и запустится сессия Spark, то есть мы будем иметь доступ к спарковскому API. 
```sh
[justribentrop_cloud@sbt-scala-spark-2 ~]$ spark-shell
...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://sbt-scala-spark-2.us-central1-a.c.inspired-muse-262209.internal:4040
Spark context available as 'sc' (master = local[*], app id = local-1578734564827).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/

Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_232)
Type in expressions to have them evaluated.
Type :help for more information.
```
Что здесь интересного:
- Using Spark's default log4j... - не разбирался с логированием в Spark, но, как я понял, по умолчанию Spark пишет в консоль. Для записи в файл надо настраивать конфиг, который подхватится библиотекой log4j. Смотри [spark logging to file](https://github.com/joemccann/dillinger/blob/master/KUBERNETES.md)
- Spark context Web UI available at ... -   Spark откроет "Spark shell application UI" - веб страницу нашего нашего консольного приложения, где можно посмотреть ход выполнения приложения.
- Spark context available as 'sc' ..., - Spark session available as 'spark'. - это основные сущности Spark (объекты-?) для работы нашего приложения. Вся работа со структурами данных Spark (Spark RDD, Spark Dataframe) будут происходить в контексте этих объектов. Простой код ниже это наглядно проиллюстрирует. Короче говоря, через 'sc' и 'spark' идут обращения к API.  
При написании отдельных Scala программ (OT Dispatcher, например), которые должны выполниться на Spark , но не через Spark shell (а через spark-submit при отправке приложения на кластер) нам надо будет в нашей программе явно указать, что сначала нужно создать Spark сессию и Spark контекст. Но об отдельных программах позже.

#### Spark shell
Для примера можно выполнить в spark-shell последовательно три строчки отсюда:
https://dzone.com/articles/wordcount-with-spark-and-scala
Параллельно стоит открыть UI приложеия на 4040 порту и обновлять страницу после ввода каждой из трех строк выше. 
__КАРТИНКА__
Посмотреть, сколько executors выделяет Spark под задачу (кстати, как можно менять количество?), сколько ресурсов отъедает executor (кстати, где это можно менять?)

Как можно заметить, само выполнение (непосредственно расчет) начнется только после команды __collect__. Непосредственный запуск вычислений в Spark происходит только после подачи команды "запускай вычисления, которые я тебе описал в 100500 строках выше" - в случае нашего примера это команда collect. Как я понимаю этот [подход](https://stackoverflow.com/questions/38027877/spark-transformation-why-its-lazy-and-what-is-the-advantage) к вычислением Spark (называется __lazy evaluation__) связан с тем, что в ходе вычислений Spark должен построить граф вычислений  (он же __DAG__ - алгоритм вычислений, если проще) оптимальным образом. А Spark сможет это сделать только понимая весь ход вычислений от начала и до конца. 

Вопросы lazy evaluation, термины DAG, executors, jobs, stages и т.п. для самостоятельного изучения.
Ресурсы:
XXX

#### Делаем из нашего scala проекта Spark приложение.
Адаптируем и запустим наше Scala приложение ХХХ для запуска на Spark.

spark-submit (так же запускается и OT Dispatcher)

#### Переписываем наше Spark приложение так, чтобы было обращение к Spark контексту, структурам данных Spark и т.п.
Наше приложение ХХХ по сути не содержит ничего spark-специфического. Перепишем "рабочую" приложения ХХХ

Сущности типа:
Spark DataFrame,RDD,Dataset
Spark executor, master, stage, collect и пр.

Вообще это большая тема, лучше после первого впечатления самому поискать интересные материалы.
Или же вот:

#### Возвращаемся к sbt - "более автоматическая" подготовка проекта под Spark
В разделе ХХХ мы готовили Scala проект без привязки к Spark и потом в ХХХ "руками" адаптировали его для запуска на Spark.
На самом деле с помощью sbt можно сделать проект по шаблону, который будет более "готов" для запуска на Spark.
Для этого: ХХХ
