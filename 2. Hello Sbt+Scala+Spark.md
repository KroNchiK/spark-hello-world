## 2. Hello Sbt+Scala+Spark
#### Установить Standalone spark
https://github.com/ribentrop/spark-hello-world/blob/master/spark-standalone.md
#### Работа в консоли Spark Shell
Самый простой способ заставить Spark что-то поделать, это запустить Spark Shell.
При этом запустится интерпретатор Scala и запустится сессия Spark, то есть мы будем иметь доступ к спарковскому API. 
```sh
[justribentrop_cloud@sbt-scala-spark-2 ~]$ spark-shell
...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://sbt-scala-spark-2.us-central1-a.c.inspired-muse-262209.internal:4040
Spark context available as 'sc' (master = local[*], app id = local-1578734564827).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/

Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_232)
Type in expressions to have them evaluated.
Type :help for more information.
```
Что здесь интересного:
- Using Spark's default log4j... - не разбирался с логированием в Spark, но, как я понял, по умолчанию Spark пишет в консоль. Для записи в файл надо настраивать конфиг, который подхватится библиотекой log4j. Смотри [spark logging to file](https://github.com/joemccann/dillinger/blob/master/KUBERNETES.md)
- Spark context Web UI available at ... -   Spark откроет "Spark shell application UI" - веб страницу нашего нашего консольного приложения, где можно посмотреть ход выполнения приложения.
- Spark context available as 'sc' ..., - Spark session available as 'spark'. - это основные сущности Spark (объекты-?) для работы нашего приложения. Вся работа со структурами данных Spark (Spark RDD, Spark Dataframe) будут происходить в контексте этих объектов. Простой код ниже это наглядно проиллюстрирует. Короче говоря, через 'sc' и 'spark' идут обращения к API.  
При написании отдельных Scala программ (OT Dispatcher, например), которые должны выполниться на Spark , но не через Spark shell (а через spark-submit при отправке приложения на кластер) нам надо будет в нашей программе явно указать, что сначала нужно создать Spark сессию и Spark контекст. Но об отдельных программах позже.

#### Spark shell
Для примера можно выполнить в spark-shell последовательно две строчки кода [отсюда](https://dzone.com/articles/wordcount-with-spark-and-scala).
Но лучше эти две строчки превратить в три. В этом случае более нагляден принцип lazy evaluation в Spark, о котором будет ниже.
```sh
val rdd = sc.textFile("address of your file")
val res = rdd.flatMap(_.split(" ")).map(x=>(x,1)).reduceByKey(_ + _)
res.collect // или res.collect()
```
Параллельно стоит открыть UI приложеия на 4040 порту и обновлять страницу после ввода каждой из трех строк выше. 
Посмотреть, сколько executors выделяет Spark под задачу, сколько ресурсов отъедает executor.

Как можно заметить, само выполнение (непосредственно расчет) начнется только после команды __collect__. Непосредственный запуск вычислений в Spark происходит только после подачи команды "запускай вычисления, которые я тебе описал  выше" - в случае нашего примера это команда collect. Как я понимаю этот [подход](https://stackoverflow.com/questions/38027877/spark-transformation-why-its-lazy-and-what-is-the-advantage) к вычислением Spark (называется __lazy evaluation__) связан с тем, что в ходе вычислений Spark должен построить граф вычислений  (он же __DAG__ - алгоритм вычислений, если проще) оптимальным образом. А Spark сможет это сделать только понимая весь ход вычислений от начала и до конца. 

Вопросы lazy evaluation, термины DAG, executors, jobs, stages и т.п. для самостоятельного изучения.
Ресурсы:
https://medium.com/@goyalsaurabh66/spark-basics-rdds-stages-tasks-and-dag-8da0f52f0454
https://stackoverflow.com/questions/42263270/what-is-the-concept-of-application-job-stage-and-task-in-spark
https://stackoverflow.com/questions/37528047/how-are-stages-split-into-tasks-in-spark

#### Делаем из нашего scala проекта Spark приложение.
Адаптируем и запустим, созданное в Sbt+Scala приложение Scala для запуска на Spark.
Для этого вокруг нашего единственного исполняемоего "println("Hello")" делаем следующую обертку (ССЫЛКУ НА ПОЯСНЕНИЕ)
```sh
object WordCount {
  def main(args: Array[String]): Unit = {
    println("Hello")
  }
}
```
Запуск: https://spark.apache.org/docs/latest/quick-start.html
```sh
spark-submit \
--class WordCount \
--master local \
/home/justribentrop_cloud/foo-build/target/scala-2.12/classes/WordCount.class

/home/justribentrop_cloud/foo-build/target/scala-2.12/foo-build_2.12-0.1.0-SNAPSHOT.jar

--master spark://10.128.0.41:7077 \
/home/justribentrop_cloud/foo-build/target/scala-2.12/foo-build_2.12-0.1.0-SNAPSHOT.jar
```


#### Переписываем наше Spark приложение так, чтобы было обращение к Spark контексту, структурам данных Spark и т.п.
Наше приложение ХХХ по сути не содержит ничего spark-специфического. Перепишем "рабочую" приложения ХХХ

Сущности типа:
Spark DataFrame,RDD,Dataset
Spark executor, master, stage, collect и пр.

Вообще это большая тема, лучше после первого впечатления самому поискать интересные материалы.
Или же вот:

#### Возвращаемся к sbt - "более автоматическая" подготовка проекта под Spark
В разделе ХХХ мы готовили Scala проект без привязки к Spark и потом в ХХХ "руками" адаптировали его для запуска на Spark.
На самом деле с помощью sbt можно сделать проект по шаблону, который будет более "готов" для запуска на Spark.
Для этого: ХХХ

==========


import org.apache.spark.sql.SparkSession

object WordCount {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().master(master = "local").getOrCreate()

    val sc = spark.sparkContext

    val rdd = sc.textFile("D:\\1.txt")

    val counts = rdd.flatMap(_.split(" ")).map(x => (x, 1)).reduceByKey(_ + _).collect()

    println(counts.toList)
  }
}

package example

object Hello extends App {
  println("Hello")
}

============

name := "WordCount"

version := "1.0"

scalaVersion := "2.11.12"

libraryDependencies += "org.apache.spark" %% "spark-core" % "2.2.0"
libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.4"
